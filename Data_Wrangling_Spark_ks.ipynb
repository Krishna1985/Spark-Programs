{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d81c70",
   "metadata": {},
   "source": [
    "### Data Wrangling with Spark - My practise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1665ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, asc, desc\n",
    "from pyspark.sql.functions import sum as Fsum # sum(), min(), max() here are the built-in functions\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14c494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession Object\n",
    "spark = SparkSession.builder.appName(\"Wrangling Data with Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e5a21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\krish\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe',\n",
       " 'C:\\\\Users\\\\krish\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe',\n",
       " 'C:\\\\Users\\\\krish\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe',\n",
       " sys.version_info(major=3, minor=10, micro=1, releaselevel='final', serial=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'],os.environ['PYSPARK_PYTHON'],sys.executable,sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9484b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulating the os.environment variables i.e. 'PYSPARK_PYTHON', 'PYSPARK_DRIVER_PYTHON'\n",
    "# import os,sys\n",
    "# print(os.environ['PYSPARK_PYTHON'],'\\n',os.environ['PYSPARK_DRIVER_PYTHON'],'\\n',sys.executable)\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# print(os.environ['PYSPARK_PYTHON'],'\\n',os.environ['PYSPARK_DRIVER_PYTHON'],'\\n',sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a5136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json log file into a Spark Data frame\n",
    "input_path = \"data/sparkify_log_small.json\"\n",
    "user_log_df = spark.read.json(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e28cc",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "- All next cells to slice, dice and explore the above Spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc926fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|              artist|     auth|firstName|gender|itemInSession| lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+--------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       Showaddywaddy|Logged In|  Kenneth|     M|          112| Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "|          Lily Allen|Logged In|Elizabeth|     F|            7|    Chase|195.23873| free|Shreveport-Bossie...|   PUT|NextSong|1512718541284|     5027|       Cheryl Tweedy|   200|1513720878284|\"Mozilla/5.0 (Win...|  1000|\n",
      "|Cobra Starship Fe...|Logged In|     Vera|     F|            6|Blackwell|196.20526| paid|          Racine, WI|   PUT|NextSong|1499855749284|     5516|Good Girls Go Bad...|   200|1513720881284|\"Mozilla/5.0 (Mac...|  2219|\n",
      "|          Alex Smoke|Logged In|   Sophee|     F|            8|   Barker|405.99465| paid|San Luis Obispo-P...|   PUT|NextSong|1513009647284|     2372| Don't See The Point|   200|1513720905284|\"Mozilla/5.0 (Win...|  2373|\n",
      "|                null|Logged In|   Jordyn|     F|            0|    Jones|     null| free|        Syracuse, NY|   GET|    Home|1513648531284|     1746|                null|   200|1513720913284|\"Mozilla/5.0 (Mac...|  1747|\n",
      "|                null|Logged In|   Jordyn|     F|            1|    Jones|     null| free|        Syracuse, NY|   GET|Settings|1513648531284|     1746|                null|   200|1513720932284|\"Mozilla/5.0 (Mac...|  1747|\n",
      "|              Redman|Logged In|    Paige|     F|            0|   Hunter|154.53995| free|       Brownwood, TX|   PUT|NextSong|1498414068284|     4406|        Smoke Buddah|   200|1513720955284|\"Mozilla/5.0 (Mac...|  1162|\n",
      "|     Ulrich Schnauss|Logged In|  Gabriel|     M|            2|     Koch|402.93832| paid|     Panama City, FL|   PUT|NextSong|1505820418284|     1060|           On My Own|   200|1513720959284|Mozilla/5.0 (Wind...|  1061|\n",
      "|                null|Logged In|    Mason|     M|            2|   Thomas|     null| paid|Indianapolis-Carm...|   GET|    Home|1487015656284|     5661|                null|   200|1513720959284|Mozilla/5.0 (comp...|   748|\n",
      "|                null|Logged In|Alexander|     M|            0|    Short|     null| free|       Green Bay, WI|   GET|    Home|1513594398284|     3689|                null|   200|1513720980284|\"Mozilla/5.0 (Win...|   597|\n",
      "|               Jay-Z|Logged In|  Micheal|     M|           23|   Morgan|223.26812| paid|        Sterling, CO|   PUT|NextSong|1485051492284|     5175|Heart Of The City...|   200|1513720983284|\"Mozilla/5.0 (Win...|  1806|\n",
      "|         Evanescence|Logged In|    Mason|     M|            3|   Thomas|237.11302| paid|Indianapolis-Carm...|   PUT|NextSong|1487015656284|     5661|    Bring Me To Life|   200|1513720993284|Mozilla/5.0 (comp...|   748|\n",
      "|     Scissor Sisters|Logged In|   Justin|     M|           82|    Jones|218.01751| paid|          Dayton, TN|   PUT|NextSong|1504706730284|     1175|               Laura|   200|1513721031284|\"Mozilla/5.0 (Win...|  1176|\n",
      "|        3 Doors Down|Logged In|     Zoie|     F|           28|   Wright|237.13914| paid|Chicago-Napervill...|   PUT|NextSong|1512172030284|     2163|    Here Without You|   200|1513721045284|\"Mozilla/5.0 (Mac...|  2164|\n",
      "|       George Younce|Logged In|  Malachi|     M|            3|     Lane|191.68608| free|     Sioux Falls, SD|   PUT|NextSong|1510109243284|     5272|This Old House w/...|   200|1513721058284|Mozilla/5.0 (Wind...|  2146|\n",
      "|              Aly-Us|Logged In|     Vera|     F|            7|Blackwell|383.58159| paid|          Racine, WI|   PUT|NextSong|1499855749284|     5516|Follow Me (Club Mix)|   200|1513721077284|\"Mozilla/5.0 (Mac...|  2219|\n",
      "|                null|Logged In|   Justin|     M|           83|    Jones|     null| paid|          Dayton, TN|   GET|    Home|1504706730284|     1175|                null|   200|1513721088284|\"Mozilla/5.0 (Win...|  1176|\n",
      "|            BjÃÂ¶rk|Logged In|    Elena|     F|            0|   Newman|348.57751| free|Boston-Cambridge-...|   PUT|NextSong|1513494341284|     2903|                Undo|   200|1513721095284|Mozilla/5.0 (Wind...|  2904|\n",
      "|      David Bromberg|Logged In|Alexander|     M|            1|    Short|146.36363| free|       Green Bay, WI|   PUT|NextSong|1513594398284|     3689|Sheebeg And Sheemore|   200|1513721097284|\"Mozilla/5.0 (Win...|   597|\n",
      "|          Nickelback|Logged In|  Abigail|     F|            4| Davidson|238.18404| paid|Lansing-East Lans...|   PUT|NextSong|1513173389284|     4591|Far Away (Album V...|   200|1513721104284|\"Mozilla/5.0 (Mac...|   226|\n",
      "+--------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See 4 rows of the data frame\n",
    "# user_log_df.take(4)\n",
    "user_log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87c1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the data frame\n",
    "user_log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ee9f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+---------+------+------------------+--------+-----------------+-----+------------+------+-------+--------------------+------------------+--------+-----------------+-------------------+--------------------+------------------+\n",
      "|summary|           artist|      auth|firstName|gender|     itemInSession|lastName|           length|level|    location|method|   page|        registration|         sessionId|    song|           status|                 ts|           userAgent|            userId|\n",
      "+-------+-----------------+----------+---------+------+------------------+--------+-----------------+-----+------------+------+-------+--------------------+------------------+--------+-----------------+-------------------+--------------------+------------------+\n",
      "|  count|             8347|     10000|     9664|  9664|             10000|    9664|             8347|10000|        9664| 10000|  10000|                9664|             10000|    8347|            10000|              10000|                9664|             10000|\n",
      "|   mean|            461.0|      null|     null|  null|           19.6734|    null|249.6486587492506| null|        null|  null|   null|1.504695369588739...|         4436.7511|Infinity|         202.8984| 1.5137859954164E12|                null|1442.4413286423842|\n",
      "| stddev|            300.0|      null|     null|  null|25.382114916132608|    null|95.00437130781461| null|        null|  null|   null|  8.47314252131656E9|2043.1281541827561|     NaN|18.04179115450588|3.290828862357974E7|                null| 829.8909432082621|\n",
      "|    min|              !!!|     Guest|   Aakash|     F|                 0| Acevedo|          1.12281| free|Aberdeen, WA|   GET|  About|       1463503881284|                 9|      #1|              200|      1513720872284|\"Mozilla/5.0 (Mac...|                  |\n",
      "|    max|ÃÂlafur Arnalds|Logged Out|     Zoie|     M|               163|  Zuniga|        1806.8371| paid|    Yuma, AZ|   PUT|Upgrade|       1513760702284|              7144|wingless|              404|      1513848349284|Mozilla/5.0 (comp...|               999|\n",
      "+-------+-----------------+----------+---------+------+------------------+--------+-----------------+-----+------------+------+-------+--------------------+------------------+--------+-----------------+-------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the data frame i.e. to see the statistics of the numerical columns\n",
    "user_log_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7803fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           artist|\n",
      "+-------+-----------------+\n",
      "|  count|             8347|\n",
      "|   mean|            461.0|\n",
      "| stddev|            300.0|\n",
      "|    min|              !!!|\n",
      "|    max|ÃÂlafur Arnalds|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us describe just one column of the data frame\n",
    "user_log_df.describe(\"artist\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b32d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         sessionId|\n",
      "+-------+------------------+\n",
      "|  count|             10000|\n",
      "|   mean|         4436.7511|\n",
      "| stddev|2043.1281541827561|\n",
      "|    min|                 9|\n",
      "|    max|              7144|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_df.describe('sessionId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c9f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count the rows in the data frame\n",
    "user_log_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ef0f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|           About|\n",
      "|       Downgrade|\n",
      "|           Error|\n",
      "|            Help|\n",
      "|            Home|\n",
      "|           Login|\n",
      "|          Logout|\n",
      "|        NextSong|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|Submit Downgrade|\n",
      "|  Submit Upgrade|\n",
      "|         Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select 'page' column, drop the duplicates in 'page'column and sort them \n",
    "user_log_df.select('page').dropDuplicates().sort('page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03418a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------------------+\n",
      "|userId|firstname|    page|                song|\n",
      "+------+---------+--------+--------------------+\n",
      "|  1046|  Kenneth|NextSong|Christmas Tears W...|\n",
      "|  1046|  Kenneth|NextSong|  Be Wary Of A Woman|\n",
      "|  1046|  Kenneth|NextSong|   Public Enemy No.1|\n",
      "|  1046|  Kenneth|NextSong|Reign Of The Tyrants|\n",
      "|  1046|  Kenneth|NextSong|      Father And Son|\n",
      "|  1046|  Kenneth|NextSong|               No. 5|\n",
      "|  1046|  Kenneth|NextSong|           Seventeen|\n",
      "|  1046|  Kenneth|    Home|                null|\n",
      "|  1046|  Kenneth|NextSong|          War on war|\n",
      "|  1046|  Kenneth|NextSong|   Killermont Street|\n",
      "|  1046|  Kenneth|NextSong|        Black & Blue|\n",
      "|  1046|  Kenneth|  Logout|                null|\n",
      "|  1046|  Kenneth|    Home|                null|\n",
      "|  1046|  Kenneth|NextSong|     Heads Will Roll|\n",
      "|  1046|  Kenneth|NextSong|Bleed It Out [Liv...|\n",
      "|  1046|  Kenneth|NextSong|              Clocks|\n",
      "|  1046|  Kenneth|NextSong|           Love Rain|\n",
      "|  1046|  Kenneth|NextSong|Ry Ry's Song (Alb...|\n",
      "|  1046|  Kenneth|NextSong|   The Invisible Man|\n",
      "|  1046|  Kenneth|NextSong|Catch You Baby (S...|\n",
      "+------+---------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select multiple fields and filter out 1046 userID\n",
    "user_log_df.select(['userId','firstname','page','song']).where(user_log_df.userId == 1046).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2412244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------------------+\n",
      "|userId|firstname|    page|                song|\n",
      "+------+---------+--------+--------------------+\n",
      "|  1046|  Kenneth|NextSong|Christmas Tears W...|\n",
      "|  1046|  Kenneth|NextSong|  Be Wary Of A Woman|\n",
      "|  1046|  Kenneth|NextSong|   Public Enemy No.1|\n",
      "|  1046|  Kenneth|NextSong|Reign Of The Tyrants|\n",
      "|  1046|  Kenneth|NextSong|      Father And Son|\n",
      "|  1046|  Kenneth|NextSong|               No. 5|\n",
      "|  1046|  Kenneth|NextSong|           Seventeen|\n",
      "|  1046|  Kenneth|NextSong|          War on war|\n",
      "|  1046|  Kenneth|NextSong|   Killermont Street|\n",
      "|  1046|  Kenneth|NextSong|        Black & Blue|\n",
      "|  1046|  Kenneth|NextSong|     Heads Will Roll|\n",
      "|  1046|  Kenneth|NextSong|Bleed It Out [Liv...|\n",
      "|  1046|  Kenneth|NextSong|              Clocks|\n",
      "|  1046|  Kenneth|NextSong|           Love Rain|\n",
      "|  1046|  Kenneth|NextSong|Ry Ry's Song (Alb...|\n",
      "|  1046|  Kenneth|NextSong|   The Invisible Man|\n",
      "|  1046|  Kenneth|NextSong|Catch You Baby (S...|\n",
      "|  1046|  Kenneth|NextSong|   Ask The Mountains|\n",
      "|  1046|  Kenneth|NextSong|Given Up (Album V...|\n",
      "|  1046|  Kenneth|NextSong|         El Cuatrero|\n",
      "+------+---------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also use 'filter' function as an alias to 'where'\n",
    "user_log_df.select(['userId','firstname','page','song']).filter((user_log_df.userId == 1046) & (user_log_df.page == 'NextSong')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb68cef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='1046', firstname='Kenneth', page='NextSong', song='Christmas Tears Will Fall'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Be Wary Of A Woman'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Public Enemy No.1'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Reign Of The Tyrants'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Father And Son'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='No. 5'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Seventeen'),\n",
       " Row(userId='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='War on war'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Killermont Street'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Black & Blue'),\n",
       " Row(userId='1046', firstname='Kenneth', page='Logout', song=None),\n",
       " Row(userId='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Heads Will Roll'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Bleed It Out [Live At Milton Keynes]'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Clocks'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Love Rain'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song=\"Ry Ry's Song (Album Version)\"),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='The Invisible Man'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Ask The Mountains'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Given Up (Album Version)'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='El Cuatrero'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Hero/Heroine'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Spring'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Rising Moon'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Tough Little Boys'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song=\"Qu'Est-Ce Que T'Es Belle\"),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Secrets'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Under The Gun')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us actually start an action instead of just displaying records using show() function above\n",
    "user_log_df.select(['userId','firstname','page','song']).filter(user_log_df.userId == 1046).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b95242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us understand what is the data type of the above output\n",
    "x = user_log_df.select(['userId','firstname','page','song']).filter(user_log_df.userId == 1046).collect()\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad8f92",
   "metadata": {},
   "source": [
    "#### Calculating statistics by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-defined function which gives hour value from the input given to it\n",
    "get_hour = udf(lambda x : x.datetime.datetime.fromtimestamp(x/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called 'hour' to the Spark data frame\n",
    "# withColumn() of a data frame needs the name of the new column followed by the expression how to compute it\n",
    "user_log_df = user_log_df.withColumn('hour',get_hour(user_log_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df09954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f23b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_log_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "318eb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the head of it\n",
    "songs_in_hour = user_log_df.filter(user_log_df.page == \"NextSong\").groupby(user_log_df.hour).count().orderBy(user_log_df.hour.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1b4753c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# songs_in_hour.show()\n",
    "type(songs_in_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e11187-fc1b-4010-a4a3-2253b04409cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o101.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 1 times, most recent failure: Lost task 1.0 in stage 16.0 (TID 19) (10.0.0.152 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m songs_in_hour_pd \u001b[38;5;241m=\u001b[39m \u001b[43msongs_in_hour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m songs_in_hour_pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(songs_in_hour_pd\u001b[38;5;241m.\u001b[39mhour)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \n\u001b[0;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[1;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o101.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 1 times, most recent failure: Lost task 1.0 in stage 16.0 (TID 19) (10.0.0.152 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "songs_in_hour_pd['hour'] = pd.to_numeric(songs_in_hour_pd.hour)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
