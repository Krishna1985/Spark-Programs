{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d81c70",
   "metadata": {},
   "source": [
    "### Data Wrangling with Spark - My practise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1665ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, asc, desc\n",
    "from pyspark.sql.functions import sum as Fsum # sum(), min(), max() here are the built-in functions\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14c494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession Object\n",
    "spark = SparkSession.builder.appName(\"Wrangling Data with Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e5a21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\krish\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe',\n",
       " 'C:\\\\Users\\\\krish\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe',\n",
       " 'C:\\\\Users\\\\krish\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python.exe',\n",
       " sys.version_info(major=3, minor=10, micro=1, releaselevel='final', serial=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'],os.environ['PYSPARK_PYTHON'],sys.executable,sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334cefce-47df-4334-b665-f2a085f53794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file as pandas data frame, convert this pandas dataframe into a csv file\n",
    "import pandas as pd\n",
    "p_df = pd.read_json(r'data/sparkify_log_small.json',lines=True)\n",
    "p_df.to_csv(r'data/sparkify_log_small.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9484b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulating the os.environment variables i.e. 'PYSPARK_PYTHON', 'PYSPARK_DRIVER_PYTHON'\n",
    "# import os,sys\n",
    "# print(os.environ['PYSPARK_PYTHON'],'\\n',os.environ['PYSPARK_DRIVER_PYTHON'],'\\n',sys.executable)\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# print(os.environ['PYSPARK_PYTHON'],'\\n',os.environ['PYSPARK_DRIVER_PYTHON'],'\\n',sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a5136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json log file into a Spark Data frame\n",
    "input_path = \"data/sparkify_log_small.csv\"\n",
    "user_log_df = spark.read.csv(input_path,sep=',',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e28cc",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "- All next cells to slice, dice and explore the above Spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc926fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+---------+--------+---------+------+------+-----+-------------+--------------------+--------------------+--------------------+---------+---------------+---------------+---------------+--------------------+--------------------+\n",
      "|           ts|userId|sessionId|    page|     auth|method|status|level|itemInSession|            location|           userAgent|            lastName|firstName|   registration|         gender|         artist|                song|              length|\n",
      "+-------------+------+---------+--------+---------+------+------+-----+-------------+--------------------+--------------------+--------------------+---------+---------------+---------------+---------------+--------------------+--------------------+\n",
      "|1513720872284|  1046|     5132|NextSong|Logged In|   PUT|   200| paid|          112|Charlotte-Concord...|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...| Matthews|        Kenneth|1509380319284.0|              M|       Showaddywaddy|Christmas Tears W...|\n",
      "|1513720878284|  1000|     5027|NextSong|Logged In|   PUT|   200| free|            7|Shreveport-Bossie...|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|    Chase|      Elizabeth|1512718541284.0|              F|          Lily Allen|       Cheryl Tweedy|\n",
      "|1513720881284|  2219|     5516|NextSong|Logged In|   PUT|   200| paid|            6|          Racine, WI|\"\"\"Mozilla/5.0 (M...| like Gecko) Vers...|Blackwell|           Vera|1499855749284.0|              F|Cobra Starship Fe...|Good Girls Go Bad...|\n",
      "|1513720905284|  2373|     2372|NextSong|Logged In|   PUT|   200| paid|            8|San Luis Obispo-P...|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|   Barker|         Sophee|1513009647284.0|              F|          Alex Smoke| Don't See The Point|\n",
      "|1513720913284|  1747|     1746|    Home|Logged In|   GET|   200| free|            0|        Syracuse, NY|\"\"\"Mozilla/5.0 (M...| like Gecko) Chro...|    Jones|         Jordyn|1513648531284.0|              F|                null|                null|\n",
      "|1513720932284|  1747|     1746|Settings|Logged In|   GET|   200| free|            1|        Syracuse, NY|\"\"\"Mozilla/5.0 (M...| like Gecko) Chro...|    Jones|         Jordyn|1513648531284.0|              F|                null|                null|\n",
      "|1513720955284|  1162|     4406|NextSong|Logged In|   PUT|   200| free|            0|       Brownwood, TX|\"\"\"Mozilla/5.0 (M...| like Gecko) Chro...|   Hunter|          Paige|1498414068284.0|              F|              Redman|        Smoke Buddah|\n",
      "|1513720959284|  1061|     1060|NextSong|Logged In|   PUT|   200| paid|            2|     Panama City, FL|Mozilla/5.0 (Wind...|                Koch|  Gabriel|1505820418284.0|              M|Ulrich Schnauss|           On My Own|           402.93832|\n",
      "|1513720959284|   748|     5661|    Home|Logged In|   GET|   200| paid|            2|Indianapolis-Carm...|Mozilla/5.0 (comp...|              Thomas|    Mason|1487015656284.0|              M|           null|                null|                null|\n",
      "|1513720980284|   597|     3689|    Home|Logged In|   GET|   200| free|            0|       Green Bay, WI|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|    Short|      Alexander|1513594398284.0|              M|                null|                null|\n",
      "|1513720983284|  1806|     5175|NextSong|Logged In|   PUT|   200| paid|           23|        Sterling, CO|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|   Morgan|        Micheal|1485051492284.0|              M|               Jay-Z|Heart Of The City...|\n",
      "|1513720993284|   748|     5661|NextSong|Logged In|   PUT|   200| paid|            3|Indianapolis-Carm...|Mozilla/5.0 (comp...|              Thomas|    Mason|1487015656284.0|              M|    Evanescence|    Bring Me To Life|           237.11302|\n",
      "|1513721031284|  1176|     1175|NextSong|Logged In|   PUT|   200| paid|           82|          Dayton, TN|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|    Jones|         Justin|1504706730284.0|              M|     Scissor Sisters|               Laura|\n",
      "|1513721045284|  2164|     2163|NextSong|Logged In|   PUT|   200| paid|           28|Chicago-Napervill...|\"\"\"Mozilla/5.0 (M...| like Gecko) Chro...|   Wright|           Zoie|1512172030284.0|              F|        3 Doors Down|    Here Without You|\n",
      "|1513721058284|  2146|     5272|NextSong|Logged In|   PUT|   200| free|            3|     Sioux Falls, SD|Mozilla/5.0 (Wind...|                Lane|  Malachi|1510109243284.0|              M|  George Younce|This Old House w/...|           191.68608|\n",
      "|1513721077284|  2219|     5516|NextSong|Logged In|   PUT|   200| paid|            7|          Racine, WI|\"\"\"Mozilla/5.0 (M...| like Gecko) Vers...|Blackwell|           Vera|1499855749284.0|              F|              Aly-Us|Follow Me (Club Mix)|\n",
      "|1513721088284|  1176|     1175|    Home|Logged In|   GET|   200| paid|           83|          Dayton, TN|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|    Jones|         Justin|1504706730284.0|              M|                null|                null|\n",
      "|1513721095284|  2904|     2903|NextSong|Logged In|   PUT|   200| free|            0|Boston-Cambridge-...|Mozilla/5.0 (Wind...|              Newman|    Elena|1513494341284.0|              F|       BjÃÂ¶rk|                Undo|           348.57751|\n",
      "|1513721097284|   597|     3689|NextSong|Logged In|   PUT|   200| free|            1|       Green Bay, WI|\"\"\"Mozilla/5.0 (W...| like Gecko) Chro...|    Short|      Alexander|1513594398284.0|              M|      David Bromberg|Sheebeg And Sheemore|\n",
      "|1513721104284|   226|     4591|NextSong|Logged In|   PUT|   200| paid|            4|Lansing-East Lans...|\"\"\"Mozilla/5.0 (M...| like Gecko) Vers...| Davidson|        Abigail|1513173389284.0|              F|          Nickelback|Far Away (Album V...|\n",
      "+-------------+------+---------+--------+---------+------+------+-----+-------------+--------------------+--------------------+--------------------+---------+---------------+---------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See 4 rows of the data frame\n",
    "# user_log_df.take(4)\n",
    "user_log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b87c1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- sessionId: integer (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- registration: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the data frame\n",
    "user_log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30ee9f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+-------+----------+------+------------------+-----+------------------+------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                  ts|            userId|         sessionId|   page|      auth|method|            status|level|     itemInSession|    location|           userAgent|            lastName|firstName|        registration|              gender|              artist|                song|              length|\n",
      "+-------+--------------------+------------------+------------------+-------+----------+------+------------------+-----+------------------+------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|               10000|              9664|             10000|  10000|     10000| 10000|             10000|10000|             10000|        9664|                9664|                9664|     9664|                9664|                9664|                9275|                8347|                8347|\n",
      "|   mean|  1.5137859954164E12|1442.4413286423842|         4436.7511|   null|      null|  null|          202.8984| null|           19.6734|        null|                null|                null|     null|1.506682696831957...|1.503662484459184...|               311.0|   773.6666666666666|            Infinity|\n",
      "| stddev|3.2908288623601213E7| 829.8909432082613|2043.1281541827557|   null|      null|  null|18.041791154505876| null|25.382114916132597|        null|                null|                null|     null| 7.550202573091067E9| 8.739019010370161E9|                null|   789.1877153123963|                 NaN|\n",
      "|    min|       1513720872284|                 7|                 9|  About|     Guest|   GET|               200| free|                 0|Aberdeen, WA|\"\"\"Mozilla/5.0 (M...| like Gecko) Chro...|    Aaron|     1473141217284.0|     1463503881284.0|                 !!!|\"Dope Boy Magic [...|\"Amish Paradise (...|\n",
      "|    max|       1513848349284|              3002|              7144|Upgrade|Logged Out|   PUT|               404| paid|               163|    Yuma, AZ|Mozilla/5.0 (comp...|              Zuniga|   Zimora|                Zoie|                   M|the bird and the bee|   ÃÂlafur Arnalds|            wingless|\n",
      "+-------+--------------------+------------------+------------------+-------+----------+------+------------------+-----+------------------+------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the data frame i.e. to see the statistics of the numerical columns\n",
    "user_log_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7803fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           artist|\n",
      "+-------+-----------------+\n",
      "|  count|             8347|\n",
      "|   mean|            461.0|\n",
      "| stddev|            300.0|\n",
      "|    min|              !!!|\n",
      "|    max|ÃÂlafur Arnalds|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us describe just one column of the data frame\n",
    "user_log_df.describe(\"artist\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b32d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         sessionId|\n",
      "+-------+------------------+\n",
      "|  count|             10000|\n",
      "|   mean|         4436.7511|\n",
      "| stddev|2043.1281541827561|\n",
      "|    min|                 9|\n",
      "|    max|              7144|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_df.describe('sessionId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c9f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count the rows in the data frame\n",
    "user_log_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ef0f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|           About|\n",
      "|       Downgrade|\n",
      "|           Error|\n",
      "|            Help|\n",
      "|            Home|\n",
      "|           Login|\n",
      "|          Logout|\n",
      "|        NextSong|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|Submit Downgrade|\n",
      "|  Submit Upgrade|\n",
      "|         Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select 'page' column, drop the duplicates in 'page'column and sort them \n",
    "user_log_df.select('page').dropDuplicates().sort('page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03418a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------------------+\n",
      "|userId|firstname|    page|              song|\n",
      "+------+---------+--------+------------------+\n",
      "|  1046| Matthews|NextSong|     Showaddywaddy|\n",
      "|  1046| Matthews|NextSong|     Darius Rucker|\n",
      "|  1046| Matthews|NextSong|      Public Enemy|\n",
      "|  1046| Matthews|NextSong|        Jag Panzer|\n",
      "|  1046| Matthews|NextSong|           Boyzone|\n",
      "|  1046| Matthews|NextSong|  Hollywood Undead|\n",
      "|  1046| Matthews|NextSong|   Jimmy Eat World|\n",
      "|  1046| Matthews|    Home|              null|\n",
      "|  1046| Matthews|NextSong|             Wilco|\n",
      "|  1046| Matthews|NextSong|Fountains Of Wayne|\n",
      "|  1046| Matthews|NextSong|        Miike Snow|\n",
      "|  1046| Matthews|  Logout|              null|\n",
      "|  1046| Matthews|    Home|              null|\n",
      "|  1046| Matthews|NextSong|   Yeah Yeah Yeahs|\n",
      "|  1046| Matthews|NextSong|       Linkin Park|\n",
      "|  1046| Matthews|NextSong|          Coldplay|\n",
      "|  1046| Matthews|NextSong|        Jill Scott|\n",
      "|  1046| Matthews|NextSong|          Glassjaw|\n",
      "|  1046| Matthews|NextSong|     Michael Cretu|\n",
      "|  1046| Matthews|NextSong|     Lonnie Gordon|\n",
      "+------+---------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select multiple fields and filter out 1046 userID\n",
    "user_log_df.select(['userId','firstname','page','song']).where(user_log_df.userId == 1046).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2412244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------------------+\n",
      "|userId|firstname|    page|              song|\n",
      "+------+---------+--------+------------------+\n",
      "|  1046| Matthews|NextSong|     Showaddywaddy|\n",
      "|  1046| Matthews|NextSong|     Darius Rucker|\n",
      "|  1046| Matthews|NextSong|      Public Enemy|\n",
      "|  1046| Matthews|NextSong|        Jag Panzer|\n",
      "|  1046| Matthews|NextSong|           Boyzone|\n",
      "|  1046| Matthews|NextSong|  Hollywood Undead|\n",
      "|  1046| Matthews|NextSong|   Jimmy Eat World|\n",
      "|  1046| Matthews|NextSong|             Wilco|\n",
      "|  1046| Matthews|NextSong|Fountains Of Wayne|\n",
      "|  1046| Matthews|NextSong|        Miike Snow|\n",
      "|  1046| Matthews|NextSong|   Yeah Yeah Yeahs|\n",
      "|  1046| Matthews|NextSong|       Linkin Park|\n",
      "|  1046| Matthews|NextSong|          Coldplay|\n",
      "|  1046| Matthews|NextSong|        Jill Scott|\n",
      "|  1046| Matthews|NextSong|          Glassjaw|\n",
      "|  1046| Matthews|NextSong|     Michael Cretu|\n",
      "|  1046| Matthews|NextSong|     Lonnie Gordon|\n",
      "|  1046| Matthews|NextSong|          Vangelis|\n",
      "|  1046| Matthews|NextSong|       Linkin Park|\n",
      "|  1046| Matthews|NextSong|       Miguel Calo|\n",
      "+------+---------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also use 'filter' function as an alias to 'where'\n",
    "user_log_df.select(['userId','firstname','page','song']).filter((user_log_df.userId == 1046) & (user_log_df.page == 'NextSong')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb68cef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1046, firstname='Matthews', page='NextSong', song='Showaddywaddy'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Darius Rucker'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Public Enemy'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Jag Panzer'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Boyzone'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Hollywood Undead'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Jimmy Eat World'),\n",
       " Row(userId=1046, firstname='Matthews', page='Home', song=None),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Wilco'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Fountains Of Wayne'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Miike Snow'),\n",
       " Row(userId=1046, firstname='Matthews', page='Logout', song=None),\n",
       " Row(userId=1046, firstname='Matthews', page='Home', song=None),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Yeah Yeah Yeahs'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Linkin Park'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Coldplay'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Jill Scott'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Glassjaw'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Michael Cretu'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Lonnie Gordon'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Vangelis'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Linkin Park'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Miguel Calo'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Boys Like Girls'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='RMB / Talla 2XLC'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='ATB'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Gary Allan'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='Marc Lavoine;Catherine Ringer'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='OneRepublic'),\n",
       " Row(userId=1046, firstname='Matthews', page='NextSong', song='The Killers')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us actually start an action instead of just displaying records using show() function above\n",
    "user_log_df.select(['userId','firstname','page','song']).filter(user_log_df.userId == 1046).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b95242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us understand what is the data type of the above output\n",
    "x = user_log_df.select(['userId','firstname','page','song']).filter(user_log_df.userId == 1046).collect()\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad8f92",
   "metadata": {},
   "source": [
    "#### Calculating statistics by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-defined function which gives hour value from the input given to it\n",
    "get_hour = udf(lambda x : x.datetime.datetime.fromtimestamp(x/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f0f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called 'hour' to the Spark data frame\n",
    "# withColumn() of a data frame needs the name of the new column followed by the expression how to compute it\n",
    "user_log_df = user_log_df.withColumn('hour',get_hour(user_log_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df09954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- sessionId: integer (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- registration: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f23b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_log_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318eb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the head of it\n",
    "songs_in_hour = user_log_df.filter(user_log_df.page == \"NextSong\").groupby(user_log_df.hour).count().orderBy(user_log_df.hour.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b4753c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# songs_in_hour.show()\n",
    "type(songs_in_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e11187-fc1b-4010-a4a3-2253b04409cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o113.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 12) (10.0.0.152 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m songs_in_hour_pd \u001b[38;5;241m=\u001b[39m \u001b[43msongs_in_hour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m songs_in_hour_pd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(songs_in_hour_pd\u001b[38;5;241m.\u001b[39mhour)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \n\u001b[0;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[1;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o113.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 12) (10.0.0.152 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:585)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:567)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:91)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:76)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "songs_in_hour_pd['hour'] = pd.to_numeric(songs_in_hour_pd.hour)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
